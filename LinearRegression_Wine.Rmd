---
title: "Linear Regression - Predicting Wine Prices"
author: "Casandra Philipson"
date: "February 25, 2016"
output: html_document
---

Course: EdX MIT 15071x  

### Building Linear Regression Models

```{r}
wine <- read.csv("wine.csv", sep = ",")
wineTest <- read.csv("wine_test.csv", sep = ",")

#take a look at the dataset
#str(wine)
#summary(wine)

#One-Variable linear regression model with SSE
model1 <- lm(Price ~ AGST, data = wine)
SSE1 <- sum(model1$residuals^2)
summary(model1)

#Two-Variable linear regression model with SSE
model2 <- lm(Price ~ AGST + HarvestRain, data = wine)
SSE2 <- sum(model2$residuals^2)
summary(model2)

#Multiple-Variable linear regression model with SSE
model3 <- lm(Price ~ AGST + HarvestRain + Year + WinterRain + Age + FrancePop, data = wine)
SSE3 <- sum(model3$residuals^2)
summary(model3)
```

### Understanding the Model and Coefficients
* __Estimated coefficients__ are for the intercept and each indepdentent variable. A coefficient that is not significantly different from 0 doesn't positively affect the model's predictive performance.  
* __Std.Error__ is how much the coefficient could vary in value.  
* __t-value__ is Estimate/STD Error (large absolute value is good) 
* __Pr(>|t|)__ also helps estimate how valuable a variable is to the model. Small values indicate importance for predicting values. The most important variables are marked with __***__ at the end of the row. Three starts is highest significance. A __.__ is almost significant.

__Notes on performance summary for Models 1, 2 and 3:__  
Note that adding HarvestRain made the model2 better than model1. You can see this in the improved adjusted R-squared term and reduced SSE. Similarly, model3 out performed models 1 & 2.  

## Improving our model based on Coefficients
```{r}
#Multiple-Variable linear regression model with SSE
#FrancePop is only missing independent value from dataset
model4 <- lm(Price ~ AGST + HarvestRain + Year + WinterRain + Age, data = wine)
summary(model4)
SSE4 <- sum(model4$residuals^2)
```
Model4 is still better than all previous models.

__Multicolinearity__
Removing FrancePop resulted in additional variables becoming significantly important. This is caused because there is a strong correlation between two variables (i.e. Age and FrancePop). Look at the difference in correlation between variables in the wine dataset. 

Remove only one varaible at a time to opmize models!

```{r}
suppressMessages(library(ggplot2))
suppressMessages(library(gridExtra))

p1 <- ggplot(data = wine, aes(x = Age, y = FrancePop)) +
  geom_point()
p2 <- ggplot(data = wine, aes(x = HarvestRain, y = AGST)) +
  geom_point()
grid.arrange(p1, p2, ncol = 2)

#Calculate the correlation between two variables in a dataset
cor(wine$Age, wine$FrancePop)

#Calculate the correlation between all variables in a dataset
cor(wine)
```

## Model Predictive Ability
Model 4 has an R^2^ = 0.83. How well does the model perform on new data?  
__Training data__ is the data we used to build the regression model
__Test data__ is data is additional new data that the model has not been exposed to
__Out of sample accuracty__ is how well our model performs on unseen data.  


```{r}
#Predict how well the model performs on new data
predictTest <- predict(model4, newdata = wineTest)

SSE = sum((wineTest$Price - predictTest)^2)
SST = sum((wineTest$Price - mean(wine$Price))^2)
R <- 1 - SSE/SST
```

__Some considerations:__  
The test dataset only has 2 samples. This is not nearly enough to accurately determine the model's predictive ability. Additionally, if you calculate the out-of-sample R^2^ values for other models built from the training data you will find that the best model for training and test data includes _AGST, HarestRain, Age,_ and _WinterRain_ (i.e. adding the population variable is not necessarily the best).